{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on\n",
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warm-up: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41420513.00630514\n",
      "1 44881772.73801346\n",
      "2 52403661.881827965\n",
      "3 50511008.215685636\n",
      "4 34325568.738956325\n",
      "5 15788278.032777727\n",
      "6 6064683.557893049\n",
      "7 2718077.5916551803\n",
      "8 1647155.546591505\n",
      "9 1215233.815173144\n",
      "10 974456.4146416697\n",
      "11 807190.0892431247\n",
      "12 679301.9612825266\n",
      "13 577443.3578716603\n",
      "14 494876.0719067117\n",
      "15 427195.385702536\n",
      "16 371022.4900639296\n",
      "17 324065.00383904495\n",
      "18 284450.7022410064\n",
      "19 250764.47124104324\n",
      "20 222015.40426886937\n",
      "21 197317.18786830935\n",
      "22 175975.79694333405\n",
      "23 157456.02148279763\n",
      "24 141286.32481612163\n",
      "25 127105.49414794247\n",
      "26 114642.11242151276\n",
      "27 103641.96174659909\n",
      "28 93871.05368797196\n",
      "29 85188.10003730448\n",
      "30 77441.65395536847\n",
      "31 70518.87695551768\n",
      "32 64309.792985994674\n",
      "33 58733.816315638476\n",
      "34 53715.698523465944\n",
      "35 49187.69966046176\n",
      "36 45107.36350855911\n",
      "37 41428.134318276825\n",
      "38 38089.90674232443\n",
      "39 35055.914144801296\n",
      "40 32295.653871495346\n",
      "41 29779.730391080087\n",
      "42 27483.580249599163\n",
      "43 25384.87304521271\n",
      "44 23465.156123787958\n",
      "45 21708.89607628458\n",
      "46 20097.892671173737\n",
      "47 18620.430617030324\n",
      "48 17263.397329303967\n",
      "49 16015.65847806758\n",
      "50 14866.661011370787\n",
      "51 13809.521980998245\n",
      "52 12835.795540621799\n",
      "53 11938.68247244998\n",
      "54 11110.569208822573\n",
      "55 10345.303383866205\n",
      "56 9639.397088619498\n",
      "57 8986.885140729359\n",
      "58 8383.14034056125\n",
      "59 7823.644815891627\n",
      "60 7304.718380408293\n",
      "61 6823.482895660131\n",
      "62 6376.796626438536\n",
      "63 5961.771794622388\n",
      "64 5576.430408130533\n",
      "65 5217.72862395575\n",
      "66 4883.383809919739\n",
      "67 4572.168426219307\n",
      "68 4282.495608494731\n",
      "69 4012.673508944646\n",
      "70 3761.2699707745487\n",
      "71 3526.918068163193\n",
      "72 3308.44983273088\n",
      "73 3104.517977039496\n",
      "74 2914.227420312939\n",
      "75 2736.5234960422686\n",
      "76 2570.581257290014\n",
      "77 2415.5132816357236\n",
      "78 2270.528321401107\n",
      "79 2134.9759257544238\n",
      "80 2008.0659172504388\n",
      "81 1889.3389110174649\n",
      "82 1778.1556332999774\n",
      "83 1674.0444884042486\n",
      "84 1576.5200402296123\n",
      "85 1485.0977358924838\n",
      "86 1399.4284837472042\n",
      "87 1319.0342105179548\n",
      "88 1243.6114199209992\n",
      "89 1172.8249483312547\n",
      "90 1106.3807314214466\n",
      "91 1043.9849474261123\n",
      "92 985.3780484420938\n",
      "93 930.3154272743574\n",
      "94 878.5556863870784\n",
      "95 829.8870581951062\n",
      "96 784.1231238018834\n",
      "97 741.0520732191277\n",
      "98 700.5249856616365\n",
      "99 662.3895989683549\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N,D_in,H,D_out=64,1000,100,10\n",
    "\n",
    "# Create random input and output data\n",
    "x=np.random.randn(N,D_in)\n",
    "y=np.random.randn(N,D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1=np.random.randn(D_in,H)\n",
    "w2=np.random.randn(H,D_out)\n",
    "\n",
    "learning_rate=1e-6\n",
    "for t in range(100):\n",
    "    # Forward pass : compute predicted y\n",
    "    h=x.dot(w1)\n",
    "    h_relu=np.maximum(h,0)\n",
    "    y_pred=h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss=np.square(y_pred-y).sum()\n",
    "    print(t,loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with repect to loss\n",
    "    grad_y_pred=2.0*(y_pred-y)\n",
    "    grad_w2=h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu=grad_y_pred.dot(w2.T)\n",
    "    grad_h=grad_h_relu.copy()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1=x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1-=learning_rate*grad_w1\n",
    "    w2-=learning_rate*grad_w2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 133.73150634765625\n",
      "199 0.2830929756164551\n",
      "299 0.0011858056532219052\n",
      "399 6.19356069364585e-05\n",
      "499 1.8011107385973446e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype=torch.float\n",
    "device=torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N,D_in, H,D_out=64,1000,100,10\n",
    "\n",
    "# Create random input and output data\n",
    "x=torch.randn(N,D_in,device=device,dtype=dtype)\n",
    "y=torch.randn(N,D_out,device=device,dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1=torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2=torch.randn(H,D_out,device=device,dtype=dtype)\n",
    "\n",
    "learning_rate=1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h=x.mm(w1)\n",
    "    h_relu=h.clamp(min=0)\n",
    "    y_pred=h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss=(y_pred-y).pow(2).sum().item()\n",
    "    if t%100==99:\n",
    "        print(t,loss)\n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred=2.0*(y_pred-y)\n",
    "    grad_w2=h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu=grad_y_pred.mm(w2.t())\n",
    "    grad_h=grad_h_relu.clone()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1=x.t().mm(grad_h)\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1-=learning_rate*grad_w1\n",
    "    w2-=learning_rate*grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 774.7705688476562\n",
      "199 4.301462173461914\n",
      "299 0.031760912388563156\n",
      "399 0.0004948644782416523\n",
      "499 6.213696906343102e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dtype=torch.float\n",
    "device=torch.device(\"cpu\")\n",
    "\n",
    "# device=torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "# torch.backends.cuda.matmul.allow_tf32 = False # Uncomment this to run on GPU\n",
    "\n",
    "\"\"\"\n",
    " The above line disables TensorFloat32. This a feature that allows\n",
    " networks to run at a much faster speed while sacrificing precision.\n",
    " Although TensorFloat32 works well on most real models, for our toy model\n",
    " in this tutorial, the sacrificed precision causes convergence issue.\n",
    " For more information, see:\n",
    " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "\"\"\"\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "\n",
    "N,D_in,H,D_out=64,1000,100,10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass\n",
    "x=torch.randn(N,D_in,device=device,dtype=dtype)\n",
    "y=torch.randn(N,D_out,device=device,dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights\n",
    "# Setting requires_grad=True indicates that we want to compute gradients\n",
    "# with respect to these Tensors during the backward pass\n",
    "\n",
    "w1=torch.randn(D_in,H,device=device,dtype=dtype,requires_grad=True)\n",
    "w2=torch.randn(H,D_out,device=device,dtype=dtype,requires_grad=True)\n",
    "\n",
    "learning_rate=1e-6\n",
    "for t in range(500):\n",
    "    y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss=(y_pred-y).pow(2).sum()\n",
    "    if t%100==99:\n",
    "        print(t,loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1-=learning_rate*w1.grad\n",
    "        w2-=learning_rate*w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
